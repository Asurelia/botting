#!/usr/bin/env python3
"""
Test d'Int√©gration Compl√®te AMD + IA Framework
Valide l'acc√©l√©ration GPU, les performances et l'int√©gration des syst√®mes
"""

import asyncio
import time
import json
import sys
import os
from pathlib import Path
import logging
import traceback
import numpy as np

# Import des modules du framework
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AMDIntegrationTester:
    """Testeur d'int√©gration compl√®te pour AMD 7800XT"""

    def __init__(self):
        self.results = {}
        self.gpu_available = False
        self.framework_loaded = False

    async def run_full_test_suite(self):
        """Lance la suite compl√®te de tests"""
        print("üöÄ IA DOFUS - Test d'Int√©gration AMD 7800XT")
        print("=" * 60)

        test_suite = [
            ("GPU Detection", self.test_gpu_detection),
            ("PyTorch DirectML", self.test_pytorch_directml),
            ("YOLO GPU Acceleration", self.test_yolo_gpu),
            ("AI Framework Core", self.test_ai_framework),
            ("Uncertainty System", self.test_uncertainty_system),
            ("Vision Integration", self.test_vision_integration),
            ("Performance Benchmark", self.test_performance_benchmark),
            ("Memory Management", self.test_memory_management)
        ]

        for test_name, test_func in test_suite:
            print(f"\nüîÑ Test: {test_name}")
            print("-" * 40)

            try:
                start_time = time.perf_counter()
                result = await test_func()
                duration = time.perf_counter() - start_time

                self.results[test_name] = {
                    'status': 'PASS' if result else 'FAIL',
                    'duration': duration,
                    'details': getattr(result, 'details', None) if hasattr(result, 'details') else {}
                }

                status_emoji = "‚úÖ" if result else "‚ùå"
                print(f"{status_emoji} {test_name}: {'R√âUSSI' if result else '√âCHEC'} ({duration:.3f}s)")

            except Exception as e:
                self.results[test_name] = {
                    'status': 'ERROR',
                    'duration': 0,
                    'error': str(e)
                }
                print(f"üí• {test_name}: ERREUR - {e}")

        # Rapport final
        await self.generate_final_report()

    async def test_gpu_detection(self):
        """Test de d√©tection du GPU AMD"""
        try:
            import subprocess

            # D√©tection via WMI
            result = subprocess.run([
                "wmic", "path", "win32_VideoController",
                "get", "name,adapterram,driverversion"
            ], capture_output=True, text=True, shell=True)

            if result.returncode == 0:
                gpu_info = result.stdout
                print(f"Informations GPU:")
                print(gpu_info)

                if "AMD" in gpu_info or "Radeon" in gpu_info:
                    if "7800" in gpu_info:
                        print("‚úÖ AMD 7800XT d√©tect√© sp√©cifiquement")
                        return True
                    else:
                        print("‚úÖ GPU AMD d√©tect√© (mod√®le √† confirmer)")
                        return True

            return False

        except Exception as e:
            print(f"‚ùå Erreur d√©tection GPU: {e}")
            return False

    async def test_pytorch_directml(self):
        """Test PyTorch + DirectML"""
        try:
            print("Test PyTorch + DirectML...")

            # Import PyTorch
            import torch
            print(f"PyTorch version: {torch.__version__}")

            # Test DirectML
            try:
                import torch_directml
                print(f"torch-directml import√© avec succ√®s")

                if torch_directml.is_available():
                    device = torch_directml.device()
                    print(f"‚úÖ DirectML disponible - Device: {device}")

                    # Test calcul simple
                    print("Test calcul matriciel GPU...")
                    x = torch.randn(1000, 1000, device=device)
                    y = torch.randn(1000, 1000, device=device)

                    start_time = time.perf_counter()
                    z = torch.mm(x, y)
                    gpu_time = time.perf_counter() - start_time

                    print(f"‚úÖ Calcul GPU r√©ussi en {gpu_time:.4f}s")

                    # Comparaison CPU
                    x_cpu = torch.randn(1000, 1000)
                    y_cpu = torch.randn(1000, 1000)

                    start_time = time.perf_counter()
                    z_cpu = torch.mm(x_cpu, y_cpu)
                    cpu_time = time.perf_counter() - start_time

                    speedup = cpu_time / gpu_time
                    print(f"üìä Acc√©l√©ration GPU: {speedup:.2f}x plus rapide que CPU")

                    self.gpu_available = True
                    return True

                else:
                    print("‚ùå DirectML non disponible")
                    return False

            except ImportError:
                print("‚ùå torch-directml non install√©")
                return False

        except Exception as e:
            print(f"‚ùå Erreur test PyTorch: {e}")
            return False

    async def test_yolo_gpu(self):
        """Test YOLO avec acc√©l√©ration GPU"""
        try:
            print("Test YOLO GPU...")

            from ultralytics import YOLO
            import cv2

            # Chargement mod√®le YOLO
            model = YOLO('yolov8n.pt')  # Mod√®le nano pour test rapide

            # Configuration GPU
            if self.gpu_available:
                # Tentative GPU
                model.to('directml' if self.gpu_available else 'cpu')
                print("‚úÖ Mod√®le YOLO configur√© pour DirectML")
            else:
                print("‚ÑπÔ∏è Mod√®le YOLO configur√© pour CPU")

            # Cr√©ation image de test
            test_image = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)

            # Test pr√©diction
            print("Test pr√©diction YOLO...")
            start_time = time.perf_counter()
            results = model(test_image, verbose=False)
            prediction_time = time.perf_counter() - start_time

            print(f"‚úÖ Pr√©diction YOLO r√©ussie en {prediction_time:.4f}s")

            # Analyse des r√©sultats
            if len(results) > 0:
                detections = len(results[0].boxes) if results[0].boxes is not None else 0
                print(f"üìä {detections} d√©tections trouv√©es")

            return True

        except Exception as e:
            print(f"‚ùå Erreur test YOLO: {e}")
            return False

    async def test_ai_framework(self):
        """Test du Core AI Framework"""
        try:
            print("Test Core AI Framework...")

            from core.ai_framework import MetaOrchestrator, AITask, Priority

            # Cr√©ation orchestrateur
            orchestrator = MetaOrchestrator()

            # Test d√©marrage
            start_success = await orchestrator.start()
            if not start_success:
                print("‚ùå √âchec d√©marrage orchestrateur")
                return False

            print("‚úÖ Orchestrateur d√©marr√©")

            # Test soumission t√¢che
            async def test_task():
                await asyncio.sleep(0.1)
                return "test_result"

            task = AITask(
                name="test_task",
                priority=Priority.MEDIUM,
                function=test_task
            )

            submission_success = await orchestrator.submit_task(task)
            if not submission_success:
                print("‚ùå √âchec soumission t√¢che")
                return False

            print("‚úÖ T√¢che soumise avec succ√®s")

            # Attente traitement
            await asyncio.sleep(1.0)

            # V√©rification statut
            status = orchestrator.get_status()
            print(f"üìä Status orchestrateur: {json.dumps(status, indent=2)}")

            # Arr√™t propre
            await orchestrator.stop()
            print("‚úÖ Orchestrateur arr√™t√© proprement")

            self.framework_loaded = True
            return True

        except Exception as e:
            print(f"‚ùå Erreur test framework: {e}")
            traceback.print_exc()
            return False

    async def test_uncertainty_system(self):
        """Test du syst√®me d'incertitude"""
        try:
            print("Test Uncertainty Management...")

            from core.uncertainty import UncertaintyManager
            from datetime import datetime

            # Cr√©ation manager
            manager = UncertaintyManager()

            # Test donn√©es de d√©cision
            decision_data = {
                'model_output': {'confidence': 0.85, 'action': 'move'},
                'input_data': {'position': (100, 200), 'target': (150, 250)},
                'action': 'move',
                'history': [0.8, 0.82, 0.85, 0.83]
            }

            context = {
                'timestamp': datetime.now(),
                'in_combat': False,
                'players_nearby': 2,
                'hp_percentage': 95
            }

            # √âvaluation incertitude
            measurement = await manager.evaluate_decision(decision_data, context)

            print(f"‚úÖ Mesure d'incertitude calcul√©e:")
            print(f"  Confiance: {measurement.confidence_score:.3f}")
            print(f"  Niveau: {measurement.confidence_level.name}")
            print(f"  Risque: {measurement.risk_level.name}")
            print(f"  Fiabilit√©: {measurement.overall_reliability():.3f}")

            # Test recommandation
            should_proceed = await manager.should_proceed(measurement)
            print(f"  Recommandation: {'‚úÖ Proc√©der' if should_proceed else '‚ùå Arr√™ter'}")

            return True

        except Exception as e:
            print(f"‚ùå Erreur test incertitude: {e}")
            traceback.print_exc()
            return False

    async def test_vision_integration(self):
        """Test int√©gration vision hybride"""
        try:
            print("Test int√©gration vision...")

            # Test de l'existence des modules vision
            vision_modules = [
                'modules.vision.screen_analyzer',
                'modules.vision.hybrid_detector',
                'modules.vision.detection_adapter'
            ]

            for module_name in vision_modules:
                try:
                    __import__(module_name)
                    print(f"‚úÖ Module {module_name} disponible")
                except ImportError as e:
                    print(f"‚ö†Ô∏è Module {module_name} non disponible: {e}")

            # Test image synthetic
            test_image = np.random.randint(0, 255, (800, 600, 3), dtype=np.uint8)

            # Simulation analyse
            print("‚úÖ Test image synth√©tique cr√©√©e (800x600)")

            return True

        except Exception as e:
            print(f"‚ùå Erreur test vision: {e}")
            return False

    async def test_performance_benchmark(self):
        """Benchmark de performance"""
        try:
            print("Benchmark performance...")

            benchmarks = {}

            # Test calcul matriciel
            print("  - Calcul matriciel...")
            if self.gpu_available:
                import torch
                import torch_directml

                device = torch_directml.device()

                # GPU benchmark
                start_time = time.perf_counter()
                for _ in range(10):
                    x = torch.randn(500, 500, device=device)
                    y = torch.randn(500, 500, device=device)
                    z = torch.mm(x, y)
                gpu_time = time.perf_counter() - start_time

                benchmarks['matrix_gpu_10_iterations'] = gpu_time
                print(f"    GPU: {gpu_time:.4f}s")

            # CPU benchmark
            start_time = time.perf_counter()
            for _ in range(10):
                x = np.random.randn(500, 500)
                y = np.random.randn(500, 500)
                z = np.dot(x, y)
            cpu_time = time.perf_counter() - start_time

            benchmarks['matrix_cpu_10_iterations'] = cpu_time
            print(f"    CPU: {cpu_time:.4f}s")

            if self.gpu_available:
                speedup = cpu_time / gpu_time
                benchmarks['speedup_ratio'] = speedup
                print(f"    Acc√©l√©ration: {speedup:.2f}x")

            # Test async/await
            print("  - Performance async...")
            start_time = time.perf_counter()

            async def async_task():
                await asyncio.sleep(0.001)
                return True

            tasks = [async_task() for _ in range(1000)]
            await asyncio.gather(*tasks)

            async_time = time.perf_counter() - start_time
            benchmarks['async_1000_tasks'] = async_time
            print(f"    1000 t√¢ches async: {async_time:.4f}s")

            # Sauvegarde benchmarks
            benchmark_file = Path("config/performance_benchmarks.json")
            benchmark_file.parent.mkdir(exist_ok=True)
            with open(benchmark_file, 'w') as f:
                json.dump(benchmarks, f, indent=2)

            print(f"‚úÖ Benchmarks sauvegard√©s: {benchmark_file}")

            return True

        except Exception as e:
            print(f"‚ùå Erreur benchmark: {e}")
            return False

    async def test_memory_management(self):
        """Test gestion m√©moire"""
        try:
            print("Test gestion m√©moire...")

            import psutil
            import gc

            # Mesure initiale
            process = psutil.Process()
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB

            print(f"  M√©moire initiale: {initial_memory:.1f} MB")

            # Allocation importante
            print("  - Allocation m√©moire test...")
            large_arrays = []
            for i in range(10):
                array = np.random.randn(1000, 1000)
                large_arrays.append(array)

            after_allocation = process.memory_info().rss / 1024 / 1024
            print(f"  M√©moire apr√®s allocation: {after_allocation:.1f} MB")

            # Lib√©ration
            print("  - Lib√©ration m√©moire...")
            del large_arrays
            gc.collect()

            await asyncio.sleep(1.0)  # Attente pour la lib√©ration

            after_cleanup = process.memory_info().rss / 1024 / 1024
            print(f"  M√©moire apr√®s cleanup: {after_cleanup:.1f} MB")

            # V√©rification efficacit√©
            memory_freed = after_allocation - after_cleanup
            print(f"  M√©moire lib√©r√©e: {memory_freed:.1f} MB")

            efficiency = memory_freed / (after_allocation - initial_memory)
            print(f"  Efficacit√© cleanup: {efficiency:.2%}")

            return efficiency > 0.7  # 70% minimum lib√©r√©

        except Exception as e:
            print(f"‚ùå Erreur test m√©moire: {e}")
            return False

    async def generate_final_report(self):
        """G√©n√®re le rapport final"""
        print("\n" + "=" * 60)
        print("üìä RAPPORT FINAL D'INT√âGRATION")
        print("=" * 60)

        # Calcul du score global
        total_tests = len(self.results)
        passed_tests = sum(1 for r in self.results.values() if r['status'] == 'PASS')
        failed_tests = sum(1 for r in self.results.values() if r['status'] == 'FAIL')
        error_tests = sum(1 for r in self.results.values() if r['status'] == 'ERROR')

        success_rate = passed_tests / total_tests * 100

        # Affichage d√©taill√©
        for test_name, result in self.results.items():
            status_emoji = {
                'PASS': '‚úÖ',
                'FAIL': '‚ùå',
                'ERROR': 'üí•'
            }[result['status']]

            duration = result.get('duration', 0)
            print(f"{status_emoji} {test_name}: {result['status']} ({duration:.3f}s)")

            if result['status'] == 'ERROR':
                print(f"    Erreur: {result.get('error', 'Inconnue')}")

        # Score global
        print(f"\nüìà SCORE GLOBAL: {success_rate:.1f}% ({passed_tests}/{total_tests})")
        print(f"   ‚úÖ R√©ussis: {passed_tests}")
        print(f"   ‚ùå √âchecs: {failed_tests}")
        print(f"   üí• Erreurs: {error_tests}")

        # Recommandations
        print(f"\nüéØ RECOMMANDATIONS:")

        if success_rate >= 90:
            print("üöÄ EXCELLENT ! Syst√®me pr√™t pour l'IA DOFUS avanc√©e")
            print("   ‚û§ Proc√©dez √† Phase 1: Knowledge Base")
            print("   ‚û§ Lancez la consultation Gemini")
        elif success_rate >= 70:
            print("‚úÖ BON ! Syst√®me fonctionnel avec optimisations mineures")
            print("   ‚û§ Corrigez les tests √©chou√©s")
            print("   ‚û§ Proc√©dez avec pr√©caution √† Phase 1")
        elif success_rate >= 50:
            print("‚ö†Ô∏è MOYEN ! Corrections n√©cessaires avant Phase 1")
            print("   ‚û§ Priorisez les tests GPU et Framework")
            print("   ‚û§ V√©rifiez les installations")
        else:
            print("‚ùå CRITIQUE ! Corrections majeures requises")
            print("   ‚û§ Relancez setup_amd_environment.py")
            print("   ‚û§ V√©rifiez compatibilit√© syst√®me")

        # √âtapes suivantes
        print(f"\nüéØ PROCHAINES √âTAPES:")
        if success_rate >= 70:
            print("1. python scripts/gemini_consensus.py autonomy_architecture")
            print("2. Investigation Dofus Guide/Ganymede")
            print("3. Impl√©mentation Phase 1: Knowledge Graph")
        else:
            print("1. python scripts/setup_amd_environment.py")
            print("2. Correction des erreurs identifi√©es")
            print("3. Relancement du test d'int√©gration")

        # Sauvegarde rapport
        report_file = Path("config/integration_test_report.json")
        report_file.parent.mkdir(exist_ok=True)

        report_data = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'success_rate': success_rate,
            'tests': self.results,
            'recommendations': 'proceed' if success_rate >= 70 else 'fix_issues',
            'gpu_available': self.gpu_available,
            'framework_loaded': self.framework_loaded
        }

        with open(report_file, 'w') as f:
            json.dump(report_data, f, indent=2)

        print(f"\nüíæ Rapport sauvegard√©: {report_file}")

async def main():
    """Point d'entr√©e principal"""
    tester = AMDIntegrationTester()
    await tester.run_full_test_suite()

if __name__ == "__main__":
    asyncio.run(main())