#!/usr/bin/env python3
"""
Test d'IntÃ©gration ComplÃ¨te AMD + IA Framework
Valide l'accÃ©lÃ©ration GPU, les performances et l'intÃ©gration des systÃ¨mes
"""

import asyncio
import time
import json
import sys
import os
from pathlib import Path
import logging
import traceback
import numpy as np

# Import des modules du framework
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AMDIntegrationTester:
    """Testeur d'intÃ©gration complÃ¨te pour AMD 7800XT"""

    def __init__(self):
        self.results = {}
        self.gpu_available = False
        self.framework_loaded = False

    async def run_full_test_suite(self):
        """Lance la suite complÃ¨te de tests"""
        print("ğŸš€ IA DOFUS - Test d'IntÃ©gration AMD 7800XT")
        print("=" * 60)

        test_suite = [
            ("GPU Detection", self.test_gpu_detection),
            ("PyTorch DirectML", self.test_pytorch_directml),
            ("YOLO GPU Acceleration", self.test_yolo_gpu),
            ("AI Framework Core", self.test_ai_framework),
            ("Uncertainty System", self.test_uncertainty_system),
            ("Vision Integration", self.test_vision_integration),
            ("Performance Benchmark", self.test_performance_benchmark),
            ("Memory Management", self.test_memory_management)
        ]

        for test_name, test_func in test_suite:
            print(f"\nğŸ”„ Test: {test_name}")
            print("-" * 40)

            try:
                start_time = time.perf_counter()
                result = await test_func()
                duration = time.perf_counter() - start_time

                self.results[test_name] = {
                    'status': 'PASS' if result else 'FAIL',
                    'duration': duration,
                    'details': getattr(result, 'details', None) if hasattr(result, 'details') else {}
                }

                status_emoji = "âœ…" if result else "âŒ"
                print(f"{status_emoji} {test_name}: {'RÃ‰USSI' if result else 'Ã‰CHEC'} ({duration:.3f}s)")

            except Exception as e:
                self.results[test_name] = {
                    'status': 'ERROR',
                    'duration': 0,
                    'error': str(e)
                }
                print(f"ğŸ’¥ {test_name}: ERREUR - {e}")

        # Rapport final
        await self.generate_final_report()

    async def test_gpu_detection(self):
        """Test de dÃ©tection du GPU AMD"""
        try:
            import subprocess

            # DÃ©tection via WMI
            result = subprocess.run([
                "wmic", "path", "win32_VideoController",
                "get", "name,adapterram,driverversion"
            ], capture_output=True, text=True, shell=True)

            if result.returncode == 0:
                gpu_info = result.stdout
                print(f"Informations GPU:")
                print(gpu_info)

                if "AMD" in gpu_info or "Radeon" in gpu_info:
                    if "7800" in gpu_info:
                        print("âœ… AMD 7800XT dÃ©tectÃ© spÃ©cifiquement")
                        return True
                    else:
                        print("âœ… GPU AMD dÃ©tectÃ© (modÃ¨le Ã  confirmer)")
                        return True

            return False

        except Exception as e:
            print(f"âŒ Erreur dÃ©tection GPU: {e}")
            return False

    async def test_pytorch_directml(self):
        """Test PyTorch + DirectML"""
        try:
            print("Test PyTorch + DirectML...")

            # Import PyTorch
            import torch
            print(f"PyTorch version: {torch.__version__}")

            # Test DirectML
            try:
                import torch_directml
                print(f"torch-directml importÃ© avec succÃ¨s")

                if torch_directml.is_available():
                    device = torch_directml.device()
                    print(f"âœ… DirectML disponible - Device: {device}")

                    # Test calcul simple
                    print("Test calcul matriciel GPU...")
                    x = torch.randn(1000, 1000, device=device)
                    y = torch.randn(1000, 1000, device=device)

                    start_time = time.perf_counter()
                    z = torch.mm(x, y)
                    gpu_time = time.perf_counter() - start_time

                    print(f"âœ… Calcul GPU rÃ©ussi en {gpu_time:.4f}s")

                    # Comparaison CPU
                    x_cpu = torch.randn(1000, 1000)
                    y_cpu = torch.randn(1000, 1000)

                    start_time = time.perf_counter()
                    z_cpu = torch.mm(x_cpu, y_cpu)
                    cpu_time = time.perf_counter() - start_time

                    speedup = cpu_time / gpu_time
                    print(f"ğŸ“Š AccÃ©lÃ©ration GPU: {speedup:.2f}x plus rapide que CPU")

                    self.gpu_available = True
                    return True

                else:
                    print("âŒ DirectML non disponible")
                    return False

            except ImportError:
                print("âŒ torch-directml non installÃ©")
                return False

        except Exception as e:
            print(f"âŒ Erreur test PyTorch: {e}")
            return False

    async def test_yolo_gpu(self):
        """Test YOLO avec accÃ©lÃ©ration GPU"""
        try:
            print("Test YOLO GPU...")

            from ultralytics import YOLO
            import cv2

            # Chargement modÃ¨le YOLO
            model = YOLO('yolov8n.pt')  # ModÃ¨le nano pour test rapide

            # Configuration GPU
            if self.gpu_available:
                # Tentative GPU
                model.to('directml' if self.gpu_available else 'cpu')
                print("âœ… ModÃ¨le YOLO configurÃ© pour DirectML")
            else:
                print("â„¹ï¸ ModÃ¨le YOLO configurÃ© pour CPU")

            # CrÃ©ation image de test
            test_image = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)

            # Test prÃ©diction
            print("Test prÃ©diction YOLO...")
            start_time = time.perf_counter()
            results = model(test_image, verbose=False)
            prediction_time = time.perf_counter() - start_time

            print(f"âœ… PrÃ©diction YOLO rÃ©ussie en {prediction_time:.4f}s")

            # Analyse des rÃ©sultats
            if len(results) > 0:
                detections = len(results[0].boxes) if results[0].boxes is not None else 0
                print(f"ğŸ“Š {detections} dÃ©tections trouvÃ©es")

            return True

        except Exception as e:
            print(f"âŒ Erreur test YOLO: {e}")
            return False

    async def test_ai_framework(self):
        """Test du Core AI Framework"""
        try:
            print("Test Core AI Framework...")

            from core.ai_framework import MetaOrchestrator, AITask, Priority

            # CrÃ©ation orchestrateur
            orchestrator = MetaOrchestrator()

            # Test dÃ©marrage
            start_success = await orchestrator.start()
            if not start_success:
                print("âŒ Ã‰chec dÃ©marrage orchestrateur")
                return False

            print("âœ… Orchestrateur dÃ©marrÃ©")

            # Test soumission tÃ¢che
            async def test_task():
                await asyncio.sleep(0.1)
                return "test_result"

            task = AITask(
                name="test_task",
                priority=Priority.MEDIUM,
                function=test_task
            )

            submission_success = await orchestrator.submit_task(task)
            if not submission_success:
                print("âŒ Ã‰chec soumission tÃ¢che")
                return False

            print("âœ… TÃ¢che soumise avec succÃ¨s")

            # Attente traitement
            await asyncio.sleep(1.0)

            # VÃ©rification statut
            status = orchestrator.get_status()
            print(f"ğŸ“Š Status orchestrateur: {json.dumps(status, indent=2)}")

            # ArrÃªt propre
            await orchestrator.stop()
            print("âœ… Orchestrateur arrÃªtÃ© proprement")

            self.framework_loaded = True
            return True

        except Exception as e:
            print(f"âŒ Erreur test framework: {e}")
            traceback.print_exc()
            return False

    async def test_uncertainty_system(self):
        """Test du systÃ¨me d'incertitude"""
        try:
            print("Test Uncertainty Management...")

            from core.uncertainty import UncertaintyManager
            from datetime import datetime

            # CrÃ©ation manager
            manager = UncertaintyManager()

            # Test donnÃ©es de dÃ©cision
            decision_data = {
                'model_output': {'confidence': 0.85, 'action': 'move'},
                'input_data': {'position': (100, 200), 'target': (150, 250)},
                'action': 'move',
                'history': [0.8, 0.82, 0.85, 0.83]
            }

            context = {
                'timestamp': datetime.now(),
                'in_combat': False,
                'players_nearby': 2,
                'hp_percentage': 95
            }

            # Ã‰valuation incertitude
            measurement = await manager.evaluate_decision(decision_data, context)

            print(f"âœ… Mesure d'incertitude calculÃ©e:")
            print(f"  Confiance: {measurement.confidence_score:.3f}")
            print(f"  Niveau: {measurement.confidence_level.name}")
            print(f"  Risque: {measurement.risk_level.name}")
            print(f"  FiabilitÃ©: {measurement.overall_reliability():.3f}")

            # Test recommandation
            should_proceed = await manager.should_proceed(measurement)
            print(f"  Recommandation: {'âœ… ProcÃ©der' if should_proceed else 'âŒ ArrÃªter'}")

            return True

        except Exception as e:
            print(f"âŒ Erreur test incertitude: {e}")
            traceback.print_exc()
            return False

    async def test_vision_integration(self):
        """Test intÃ©gration vision hybride"""
        try:
            print("Test intÃ©gration vision...")

            # Test de l'existence des modules vision
            vision_modules = [
                'modules.vision.screen_analyzer',
                'modules.vision.hybrid_detector',
                'modules.vision.detection_adapter'
            ]

            for module_name in vision_modules:
                try:
                    __import__(module_name)
                    print(f"âœ… Module {module_name} disponible")
                except ImportError as e:
                    print(f"âš ï¸ Module {module_name} non disponible: {e}")

            # Test image synthetic
            test_image = np.random.randint(0, 255, (800, 600, 3), dtype=np.uint8)

            # Simulation analyse
            print("âœ… Test image synthÃ©tique crÃ©Ã©e (800x600)")

            return True

        except Exception as e:
            print(f"âŒ Erreur test vision: {e}")
            return False

    async def test_performance_benchmark(self):
        """Benchmark de performance"""
        try:
            print("Benchmark performance...")

            benchmarks = {}

            # Test calcul matriciel
            print("  - Calcul matriciel...")
            if self.gpu_available:
                import torch
                import torch_directml

                device = torch_directml.device()

                # GPU benchmark
                start_time = time.perf_counter()
                for _ in range(10):
                    x = torch.randn(500, 500, device=device)
                    y = torch.randn(500, 500, device=device)
                    z = torch.mm(x, y)
                gpu_time = time.perf_counter() - start_time

                benchmarks['matrix_gpu_10_iterations'] = gpu_time
                print(f"    GPU: {gpu_time:.4f}s")

            # CPU benchmark
            start_time = time.perf_counter()
            for _ in range(10):
                x = np.random.randn(500, 500)
                y = np.random.randn(500, 500)
                z = np.dot(x, y)
            cpu_time = time.perf_counter() - start_time

            benchmarks['matrix_cpu_10_iterations'] = cpu_time
            print(f"    CPU: {cpu_time:.4f}s")

            if self.gpu_available:
                speedup = cpu_time / gpu_time
                benchmarks['speedup_ratio'] = speedup
                print(f"    AccÃ©lÃ©ration: {speedup:.2f}x")

            # Test async/await
            print("  - Performance async...")
            start_time = time.perf_counter()

            async def async_task():
                await asyncio.sleep(0.001)
                return True

            tasks = [async_task() for _ in range(1000)]
            await asyncio.gather(*tasks)

            async_time = time.perf_counter() - start_time
            benchmarks['async_1000_tasks'] = async_time
            print(f"    1000 tÃ¢ches async: {async_time:.4f}s")

            # Sauvegarde benchmarks
            benchmark_file = Path("config/performance_benchmarks.json")
            benchmark_file.parent.mkdir(exist_ok=True)
            with open(benchmark_file, 'w') as f:
                json.dump(benchmarks, f, indent=2)

            print(f"âœ… Benchmarks sauvegardÃ©s: {benchmark_file}")

            return True

        except Exception as e:
            print(f"âŒ Erreur benchmark: {e}")
            return False

    async def test_memory_management(self):
        """Test gestion mÃ©moire"""
        try:
            print("Test gestion mÃ©moire...")

            import psutil
            import gc

            # Mesure initiale
            process = psutil.Process()
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB

            print(f"  MÃ©moire initiale: {initial_memory:.1f} MB")

            # Allocation importante
            print("  - Allocation mÃ©moire test...")
            large_arrays = []
            for i in range(10):
                array = np.random.randn(1000, 1000)
                large_arrays.append(array)

            after_allocation = process.memory_info().rss / 1024 / 1024
            print(f"  MÃ©moire aprÃ¨s allocation: {after_allocation:.1f} MB")

            # LibÃ©ration
            print("  - LibÃ©ration mÃ©moire...")
            del large_arrays
            gc.collect()

            await asyncio.sleep(1.0)  # Attente pour la libÃ©ration

            after_cleanup = process.memory_info().rss / 1024 / 1024
            print(f"  MÃ©moire aprÃ¨s cleanup: {after_cleanup:.1f} MB")

            # VÃ©rification efficacitÃ©
            memory_freed = after_allocation - after_cleanup
            print(f"  MÃ©moire libÃ©rÃ©e: {memory_freed:.1f} MB")

            efficiency = memory_freed / (after_allocation - initial_memory)
            print(f"  EfficacitÃ© cleanup: {efficiency:.2%}")

            return efficiency > 0.7  # 70% minimum libÃ©rÃ©

        except Exception as e:
            print(f"âŒ Erreur test mÃ©moire: {e}")
            return False

    async def generate_final_report(self):
        """GÃ©nÃ¨re le rapport final"""
        print("\n" + "=" * 60)
        print("ğŸ“Š RAPPORT FINAL D'INTÃ‰GRATION")
        print("=" * 60)

        # Calcul du score global
        total_tests = len(self.results)
        passed_tests = sum(1 for r in self.results.values() if r['status'] == 'PASS')
        failed_tests = sum(1 for r in self.results.values() if r['status'] == 'FAIL')
        error_tests = sum(1 for r in self.results.values() if r['status'] == 'ERROR')

        success_rate = passed_tests / total_tests * 100

        # Affichage dÃ©taillÃ©
        for test_name, result in self.results.items():
            status_emoji = {
                'PASS': 'âœ…',
                'FAIL': 'âŒ',
                'ERROR': 'ğŸ’¥'
            }[result['status']]

            duration = result.get('duration', 0)
            print(f"{status_emoji} {test_name}: {result['status']} ({duration:.3f}s)")

            if result['status'] == 'ERROR':
                print(f"    Erreur: {result.get('error', 'Inconnue')}")

        # Score global
        print(f"\nğŸ“ˆ SCORE GLOBAL: {success_rate:.1f}% ({passed_tests}/{total_tests})")
        print(f"   âœ… RÃ©ussis: {passed_tests}")
        print(f"   âŒ Ã‰checs: {failed_tests}")
        print(f"   ğŸ’¥ Erreurs: {error_tests}")

        # Recommandations
        print(f"\nğŸ¯ RECOMMANDATIONS:")

        if success_rate >= 90:
            print("ğŸš€ EXCELLENT ! SystÃ¨me prÃªt pour l'IA DOFUS avancÃ©e")
            print("   â¤ ProcÃ©dez Ã  Phase 1: Knowledge Base")
            print("   â¤ Lancez la consultation Gemini")
        elif success_rate >= 70:
            print("âœ… BON ! SystÃ¨me fonctionnel avec optimisations mineures")
            print("   â¤ Corrigez les tests Ã©chouÃ©s")
            print("   â¤ ProcÃ©dez avec prÃ©caution Ã  Phase 1")
        elif success_rate >= 50:
            print("âš ï¸ MOYEN ! Corrections nÃ©cessaires avant Phase 1")
            print("   â¤ Priorisez les tests GPU et Framework")
            print("   â¤ VÃ©rifiez les installations")
        else:
            print("âŒ CRITIQUE ! Corrections majeures requises")
            print("   â¤ Relancez setup_amd_environment.py")
            print("   â¤ VÃ©rifiez compatibilitÃ© systÃ¨me")

        # Ã‰tapes suivantes
        print(f"\nğŸ¯ PROCHAINES Ã‰TAPES:")
        if success_rate >= 70:
            print("1. python scripts/gemini_consensus.py autonomy_architecture")
            print("2. Investigation Dofus Guide/Ganymede")
            print("3. ImplÃ©mentation Phase 1: Knowledge Graph")
        else:
            print("1. python scripts/setup_amd_environment.py")
            print("2. Correction des erreurs identifiÃ©es")
            print("3. Relancement du test d'intÃ©gration")

        # Sauvegarde rapport
        report_file = Path("config/integration_test_report.json")
        report_file.parent.mkdir(exist_ok=True)

        report_data = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'success_rate': success_rate,
            'tests': self.results,
            'recommendations': 'proceed' if success_rate >= 70 else 'fix_issues',
            'gpu_available': self.gpu_available,
            'framework_loaded': self.framework_loaded
        }

        with open(report_file, 'w') as f:
            json.dump(report_data, f, indent=2)

        print(f"\nğŸ’¾ Rapport sauvegardÃ©: {report_file}")

async def main():
    """Point d'entrÃ©e principal"""
    tester = AMDIntegrationTester()
    await tester.run_full_test_suite()

if __name__ == "__main__":
    asyncio.run(main())