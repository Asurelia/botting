#!/usr/bin/env python3
"""
Configuration automatique de l'environnement IA pour AMD 7800XT sous Windows 11
Optimise PyTorch, OpenCV et les dÃ©pendances pour performances maximales
"""

import subprocess
import sys
import os
import platform
import json
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AMDEnvironmentSetup:
    """Configurateur d'environnement optimisÃ© AMD"""

    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.requirements_dir = self.project_root / "requirements"
        self.requirements_dir.mkdir(exist_ok=True)

        # VÃ©rification systÃ¨me
        self.system_info = self._get_system_info()

    def _get_system_info(self):
        """Collecte les informations systÃ¨me"""
        return {
            "os": platform.system(),
            "version": platform.version(),
            "architecture": platform.architecture()[0],
            "processor": platform.processor(),
            "python_version": sys.version
        }

    def check_amd_gpu(self):
        """VÃ©rifie la prÃ©sence et configuration du GPU AMD"""
        logger.info("ğŸ” VÃ©rification GPU AMD...")

        try:
            # Tentative de dÃ©tection via WMI (Windows)
            if platform.system() == "Windows":
                result = subprocess.run([
                    "wmic", "path", "win32_VideoController",
                    "get", "name,adapterram"
                ], capture_output=True, text=True, shell=True)

                if result.returncode == 0:
                    gpu_info = result.stdout
                    if "AMD" in gpu_info or "Radeon" in gpu_info:
                        logger.info("âœ… GPU AMD dÃ©tectÃ©")
                        if "7800" in gpu_info:
                            logger.info("âœ… AMD 7800XT confirmÃ© - Configuration optimisÃ©e disponible")
                            return True
                        else:
                            logger.info("âœ… GPU AMD dÃ©tectÃ© (modÃ¨le Ã  confirmer)")
                            return True
                    else:
                        logger.warning("âš ï¸ GPU AMD non dÃ©tectÃ© clairement")
                        return False

        except Exception as e:
            logger.error(f"âŒ Erreur dÃ©tection GPU: {e}")

        return False

    def create_optimized_requirements(self):
        """CrÃ©e les fichiers requirements optimisÃ©s"""
        logger.info("ğŸ“ CrÃ©ation requirements optimisÃ©s...")

        # Requirements de base IA
        base_requirements = """# IA DOFUS - Base Requirements
# OptimisÃ© pour AMD 7800XT + Windows 11

# Computer Vision & Deep Learning
ultralytics>=8.0.0
opencv-python>=4.8.0
opencv-contrib-python>=4.8.0
numpy>=1.24.0
pillow>=10.0.0

# Machine Learning & Data Science
scikit-learn>=1.3.0
pandas>=2.0.0
matplotlib>=3.7.0
seaborn>=0.12.0

# Graph Databases & Knowledge Management
networkx>=3.1
neo4j>=5.12.0
rdflib>=7.0.0

# Performance & Optimization
numba>=0.58.0
joblib>=1.3.0
cython>=3.0.0

# Async & Concurrency
aiohttp>=3.8.0

# Utilities
tqdm>=4.65.0
click>=8.1.0
pyyaml>=6.0
python-dotenv>=1.0.0
psutil>=5.9.0
"""

        # Requirements pour GPU AMD
        amd_requirements = """# IA DOFUS - AMD GPU Requirements
# Support DirectML pour AMD 7800XT

# PyTorch avec DirectML
torch>=2.0.0
torchvision>=0.15.0
torchaudio>=2.0.0
torch-directml>=0.2.0

# ONNX Runtime pour optimisation
onnxruntime-directml>=1.16.0
onnx>=1.14.0

# Alternative TensorFlow avec DirectML (optionnel)
tensorflow-directml>=1.15.8
"""

        # Requirements dÃ©veloppement
        dev_requirements = """# IA DOFUS - Development Requirements

# Testing
pytest>=7.4.0
pytest-asyncio>=0.21.0
pytest-cov>=4.1.0

# Code Quality
black>=23.7.0
isort>=5.12.0
flake8>=6.0.0
mypy>=1.5.0

# Jupyter & Analysis
jupyter>=1.0.0
ipykernel>=6.25.0
plotly>=5.15.0

# Profiling & Debugging
line_profiler>=4.1.0
memory_profiler>=0.61.0
py-spy>=0.3.14
"""

        # Sauvegarde des fichiers
        requirements_files = {
            "base.txt": base_requirements,
            "amd_gpu.txt": amd_requirements,
            "dev.txt": dev_requirements
        }

        for filename, content in requirements_files.items():
            file_path = self.requirements_dir / filename
            with open(file_path, 'w') as f:
                f.write(content)
            logger.info(f"âœ… CrÃ©Ã©: {file_path}")

        return True

    def install_base_requirements(self):
        """Installe les requirements de base"""
        logger.info("ğŸ“¦ Installation requirements de base...")

        requirements_file = self.requirements_dir / "base.txt"
        if not requirements_file.exists():
            logger.error("âŒ Fichier requirements de base non trouvÃ©")
            return False

        try:
            cmd = [sys.executable, "-m", "pip", "install", "-r", str(requirements_file)]
            result = subprocess.run(cmd, check=True, capture_output=True, text=True)
            logger.info("âœ… Requirements de base installÃ©s")
            return True
        except subprocess.CalledProcessError as e:
            logger.error(f"âŒ Erreur installation: {e.stderr}")
            return False

    def install_amd_pytorch(self):
        """Installe PyTorch optimisÃ© pour AMD"""
        logger.info("ğŸ“¦ Installation PyTorch pour AMD...")

        pytorch_commands = [
            # DÃ©sinstallation versions existantes
            [sys.executable, "-m", "pip", "uninstall", "-y", "torch", "torchvision", "torchaudio"],

            # Installation PyTorch standard + DirectML
            [sys.executable, "-m", "pip", "install", "torch", "torchvision", "torchaudio"],
            [sys.executable, "-m", "pip", "install", "torch-directml"]
        ]

        for cmd in pytorch_commands:
            try:
                logger.info(f"ExÃ©cution: {' '.join(cmd)}")
                result = subprocess.run(cmd, check=True, capture_output=True, text=True)
                logger.info("âœ… Commande rÃ©ussie")
            except subprocess.CalledProcessError as e:
                if "torch-directml" in cmd:
                    logger.error("âŒ Impossible d'installer torch-directml")
                    return False
                else:
                    logger.info("â„¹ï¸ Tentative mÃ©thode alternative...")
                    continue

        return True

    def install_amd_requirements(self):
        """Installe les requirements AMD spÃ©cifiques"""
        logger.info("ğŸ“¦ Installation requirements AMD...")

        requirements_file = self.requirements_dir / "amd_gpu.txt"
        if not requirements_file.exists():
            logger.error("âŒ Fichier requirements AMD non trouvÃ©")
            return False

        try:
            cmd = [sys.executable, "-m", "pip", "install", "-r", str(requirements_file)]
            result = subprocess.run(cmd, check=True, capture_output=True, text=True)
            logger.info("âœ… Requirements AMD installÃ©s")
            return True
        except subprocess.CalledProcessError as e:
            logger.warning(f"âš ï¸ Erreur installation AMD: {e.stderr}")
            logger.info("â„¹ï¸ Tentative installation alternative...")

            # Installation alternative une par une
            alternative_packages = [
                "onnxruntime-directml"
            ]

            for package in alternative_packages:
                try:
                    cmd = [sys.executable, "-m", "pip", "install", package]
                    subprocess.run(cmd, check=True)
                    logger.info(f"âœ… {package} installÃ©")
                except:
                    logger.warning(f"âš ï¸ Ã‰chec installation {package}")

            return True

    def test_gpu_acceleration(self):
        """Teste l'accÃ©lÃ©ration GPU"""
        logger.info("ğŸ§ª Test accÃ©lÃ©ration GPU...")

        test_script = """
import torch
import sys

print("=== Test GPU AMD ===")
print(f"PyTorch version: {torch.__version__}")

# Test CUDA (peu probable sur AMD mais on vÃ©rifie)
if torch.cuda.is_available():
    print(f"âœ… CUDA disponible - Devices: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        print(f"  Device {i}: {torch.cuda.get_device_name(i)}")
else:
    print("â„¹ï¸ CUDA non disponible (normal pour AMD)")

# Test DirectML
try:
    import torch_directml
    if torch_directml.is_available():
        device = torch_directml.device()
        print(f"âœ… DirectML disponible - Device: {device}")

        # Test simple
        x = torch.randn(1000, 1000, device=device)
        y = torch.randn(1000, 1000, device=device)
        z = torch.mm(x, y)
        print("âœ… Test calcul GPU rÃ©ussi")
        exit(0)
    else:
        print("âŒ DirectML non disponible")
except ImportError:
    print("âŒ torch-directml non installÃ©")
except Exception as e:
    print(f"âŒ Erreur DirectML: {e}")

# Test CPU en fallback
print("â„¹ï¸ Utilisation CPU en fallback")
x = torch.randn(100, 100)
y = torch.randn(100, 100)
z = torch.mm(x, y)
print("âœ… Test calcul CPU rÃ©ussi")
exit(1)
"""

        try:
            result = subprocess.run([sys.executable, "-c", test_script],
                                 capture_output=True, text=True, timeout=30)
            print(result.stdout)
            if result.stderr:
                print(f"Warnings: {result.stderr}")

            return result.returncode == 0

        except subprocess.TimeoutExpired:
            logger.error("âŒ Test GPU timeout")
            return False
        except Exception as e:
            logger.error(f"âŒ Erreur test GPU: {e}")
            return False

    def create_gpu_config(self):
        """CrÃ©e la configuration GPU optimisÃ©e"""
        logger.info("âš™ï¸ CrÃ©ation configuration GPU...")

        gpu_available = self.test_gpu_acceleration()

        config = {
            "gpu": {
                "vendor": "AMD",
                "model": "7800XT",
                "backend": "directml",
                "available": gpu_available,
                "optimization": {
                    "mixed_precision": True,
                    "memory_efficient": True,
                    "batch_size_auto": True
                }
            },
            "pytorch": {
                "device": "directml" if gpu_available else "cpu",
                "amp_enabled": True,
                "compile_enabled": True
            },
            "yolo": {
                "device": "0" if gpu_available else "cpu",
                "half": gpu_available,   # FP16 pour performance
                "optimize": True
            },
            "performance": {
                "max_workers": 8,  # AdaptÃ© au processeur
                "memory_limit_gb": 16,
                "cache_enabled": True
            }
        }

        config_file = self.project_root / "config" / "gpu_config.json"
        config_file.parent.mkdir(exist_ok=True)

        with open(config_file, 'w') as f:
            json.dump(config, f, indent=2)

        logger.info(f"âœ… Configuration GPU sauvegardÃ©e: {config_file}")
        return config

    def run_full_setup(self):
        """Lance l'installation complÃ¨te"""
        logger.info("ğŸš€ DÃ©marrage installation complÃ¨te IA DOFUS...")

        steps = [
            ("VÃ©rification GPU AMD", self.check_amd_gpu),
            ("CrÃ©ation requirements", self.create_optimized_requirements),
            ("Installation base", self.install_base_requirements),
            ("Installation PyTorch AMD", self.install_amd_pytorch),
            ("Installation AMD", self.install_amd_requirements),
            ("Configuration GPU", self.create_gpu_config)
        ]

        results = {}

        for step_name, step_func in steps:
            logger.info(f"\n{'='*50}")
            logger.info(f"ğŸ”„ {step_name}...")
            logger.info(f"{'='*50}")

            try:
                result = step_func()
                results[step_name] = result

                if result:
                    logger.info(f"âœ… {step_name} - SUCCÃˆS")
                else:
                    logger.warning(f"âš ï¸ {step_name} - Ã‰CHEC (continuant...)")

            except Exception as e:
                logger.error(f"âŒ {step_name} - ERREUR: {e}")
                results[step_name] = False

        # Rapport final
        logger.info(f"\n{'='*60}")
        logger.info("ğŸ“Š RAPPORT FINAL D'INSTALLATION")
        logger.info(f"{'='*60}")

        success_count = sum(1 for result in results.values() if result)
        total_count = len(results)

        for step, result in results.items():
            status = "âœ… SUCCÃˆS" if result else "âŒ Ã‰CHEC"
            logger.info(f"{step}: {status}")

        logger.info(f"\nScore: {success_count}/{total_count}")

        if success_count >= total_count * 0.7:  # 70% de succÃ¨s minimum
            logger.info("ğŸ‰ INSTALLATION RÃ‰USSIE ! PrÃªt pour l'IA DOFUS")
            return True
        else:
            logger.error("ğŸ’¥ INSTALLATION INCOMPLÃˆTE - VÃ©rifiez les erreurs")
            return False

def main():
    """Point d'entrÃ©e principal"""
    print("ğŸš€ IA DOFUS - Configuration Environnement AMD 7800XT")
    print("=" * 60)

    setup = AMDEnvironmentSetup()

    # Affichage info systÃ¨me
    print(f"ğŸ–¥ï¸ SystÃ¨me: {setup.system_info['os']} {setup.system_info['version']}")
    print(f"ğŸ Python: {setup.system_info['python_version']}")
    print("=" * 60)

    success = setup.run_full_setup()

    if success:
        print("\nğŸ¯ PROCHAINES Ã‰TAPES:")
        print("1. Lancez: python core/ai_framework.py --init")
        print("2. Testez: python scripts/gemini_consensus.py autonomy_architecture")
        print("3. Consultez: docs/PLAN_AUTONOMIE_ENRICHI.md")
        print("\nğŸš€ L'IA DOFUS vous attend !")
    else:
        print("\nğŸ”§ ACTIONS CORRECTIVES:")
        print("1. VÃ©rifiez les logs d'erreur ci-dessus")
        print("2. Installez manuellement les packages Ã©chouÃ©s")
        print("3. Relancez le script aprÃ¨s corrections")

if __name__ == "__main__":
    main()