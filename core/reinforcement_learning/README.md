# Syst√®me d'Apprentissage par Renforcement Avanc√© - DOFUS

## Vue d'ensemble

Ce module impl√©mente un syst√®me d'apprentissage par renforcement state-of-the-art sp√©cialement con√ßu pour l'automatisation intelligente du jeu DOFUS. Il combine plusieurs techniques avanc√©es d'IA pour cr√©er un agent adaptatif capable d'apprendre et de s'optimiser en temps r√©el.

## Fonctionnalit√©s Principales

### üß† Algorithmes RL State-of-the-Art
- **PPO (Proximal Policy Optimization)** avec optimisations avanc√©es
- **DQN (Deep Q-Network)** avec am√©liorations Rainbow
- **A3C (Asynchronous Actor-Critic)** pour entra√Ænement distribu√©
- Vision Transformer int√©gr√©e pour reconnaissance d'√©cran
- Support multi-GPU et distributed training

### üéØ Syst√®me de R√©compenses Intelligent
- R√©compenses multi-objectifs (XP, Kamas, Survie, Efficacit√©)
- Adaptation dynamique des pond√©rations
- Reward shaping pour guide l'apprentissage
- D√©tection automatique des objectifs du joueur
- Syst√®me de curriculum learning progressif

### üë®‚Äçüè´ Apprentissage par Imitation (Behavior Cloning)
- Collecte automatique des d√©monstrations utilisateur
- Preprocessing avanc√© des donn√©es d'entr√©e
- Architecture de r√©seau optimis√©e pour l'imitation
- Techniques d'augmentation des donn√©es sp√©cialis√©es
- Interface pour supervision humaine

### üöÄ Meta-Learning pour Adaptation Rapide
- Model-Agnostic Meta-Learning (MAML)
- Prototypical Networks pour few-shot learning
- Memory-Augmented Neural Networks
- Task-Conditional Networks adaptatifs
- Adaptation rapide √† de nouveaux contextes DOFUS

### üñºÔ∏è Vision Avanc√©e
- CNN + Vision Transformer hybride
- D√©tection d'√©tat du jeu automatique
- Reconnaissance d'entit√©s et d'objets
- Support multi-r√©solution et multi-fen√™tre
- Optimisations GPU pour traitement en temps r√©el

## Architecture du Syst√®me

```
core/reinforcement_learning/
‚îú‚îÄ‚îÄ __init__.py                 # Module principal
‚îú‚îÄ‚îÄ core_components.py          # Composants principaux
‚îú‚îÄ‚îÄ rl_agent.py                # Agent RL avec PPO/DQN/A3C
‚îú‚îÄ‚îÄ environment_wrapper.py     # Environnement DOFUS compatible Gym
‚îú‚îÄ‚îÄ reward_system.py           # Syst√®me de r√©compenses adaptatif
‚îú‚îÄ‚îÄ behavior_cloning.py        # Apprentissage par imitation
‚îú‚îÄ‚îÄ meta_learning.py           # Meta-learning pour adaptation
‚îú‚îÄ‚îÄ example_usage.py           # Exemples d'utilisation
‚îî‚îÄ‚îÄ README.md                  # Cette documentation
```

## Installation et D√©pendances

### Pr√©requis
- Python 3.8+
- CUDA 11.0+ (optionnel, pour GPU)
- DOFUS install√© et accessible

### D√©pendances Python
```bash
pip install torch torchvision torchaudio
pip install gymnasium
pip install numpy opencv-python pillow
pip install matplotlib seaborn
pip install h5py albumentations
pip install ultralytics  # Pour YOLO
pip install pytesseract   # Pour OCR
pip install keyboard mouse  # Pour capture d'entr√©es
pip install psutil win32gui win32ui  # Windows uniquement
pip install mss  # Pour capture d'√©cran rapide
```

## Utilisation Rapide

### 1. Entra√Ænement RL Basique

```python
from core.reinforcement_learning import RLSystem, RLSystemConfig

# Configuration
config = RLSystemConfig(
    device="cuda",
    max_episodes=1000,
    learning_rate=3e-4
)

# Syst√®me RL
rl_system = RLSystem(config)

# Entra√Ænement
history = rl_system.train(500)

# √âvaluation
results = rl_system.evaluate(20)
print(f"Performance: {results['mean_reward']:.2f} ¬± {results['std_reward']:.2f}")
```

### 2. Apprentissage par Imitation

```python
from core.reinforcement_learning.behavior_cloning import (
    DemonstrationCollector, BehaviorCloningTrainer, BehaviorCloningConfig
)

# Configuration
bc_config = BehaviorCloningConfig(batch_size=32, num_epochs=100)

# Collecte de d√©monstrations
collector = DemonstrationCollector(bc_config)
collector.start_collection()
# ... jouer normalement ...
collector.stop_collection()

# Entra√Ænement
trainer = BehaviorCloningTrainer(bc_config)
# ... (voir example_usage.py pour d√©tails complets) ...
```

### 3. Meta-Learning

```python
from core.reinforcement_learning.meta_learning import MetaLearningFactory

# Cr√©ation d'un syst√®me MAML
meta_system = MetaLearningFactory.create_maml_system(
    num_epochs=200,
    meta_batch_size=16
)

# Entra√Ænement m√©ta
history = meta_system.train_meta(100)

# Adaptation √† une nouvelle t√¢che
task = meta_system.task_generator.generate_task(TaskType.COMBAT_MONSTER)
adapted_model = meta_system.adapt_to_task(task, support_data)
```

## Configuration Avanc√©e

### Optimisations GPU

```python
config = RLSystemConfig(
    device="cuda",
    batch_size=128,        # Batch plus grand pour GPU
    hidden_dim=1024,       # R√©seau plus complexe
    use_vision_transformer=True,
    enable_meta_learning=True
)
```

### R√©glage du Syst√®me de R√©compenses

```python
from core.reinforcement_learning.reward_system import RewardConfig

reward_config = RewardConfig(
    enable_adaptation=True,
    enable_curiosity=True,
    objective_weights={
        'survival': 1.0,
        'xp_gain': 0.8,
        'efficiency': 0.6
    }
)
```

## M√©triques et Monitoring

Le syst√®me fournit des m√©triques d√©taill√©es :
- R√©compenses par √©pisode et moyennes mobiles
- Pr√©cision du behavior cloning
- Scores d'adaptation du meta-learning
- Utilisation GPU et performance
- Analyse des patterns d'action
- D√©composition des r√©compenses par objectif

## Exemples d'Utilisation

Voir `example_usage.py` pour des exemples complets incluant :
- Pipeline complet BC ‚Üí RL ‚Üí Meta-learning
- Optimisations GPU avanc√©es
- Syst√®me de r√©compenses adaptatif
- Int√©gration avec l'environnement DOFUS

## Optimisations Performance

### GPU
- Utilisation de mixed precision training
- Batch processing optimis√©
- Memory pooling pour √©viter les allocations
- Distributed training multi-GPU

### CPU
- Preprocessing asynchrone des images
- Cache intelligent des captures d'√©cran
- Threading pour I/O non-bloquant

### M√©moire
- Replay buffers avec compression
- Garbage collection optimis√©
- Memory mapping pour gros datasets

## Architecture Technique

### Agent RL
- Encoder vision CNN + Vision Transformer
- Policy et value heads s√©par√©s
- Techniques de r√©gularisation avanc√©es
- Curriculum learning int√©gr√©

### Environnement
- Interface OpenAI Gym compatible
- Capture d'√©cran multi-m√©thodes (MSS, Win32, PIL)
- D√©tection d'√©tat par vision + OCR + templates
- Actions discr√®tes et continues support√©es

### Syst√®me de R√©compenses
- Multi-objectifs avec pond√©ration adaptative
- Reward shaping bas√© sur les heuristiques DOFUS
- Curiosit√© intrins√®que pour exploration
- M√©triques de performance en temps r√©el

## Troubleshooting

### Probl√®mes Courants

1. **Fen√™tre DOFUS non d√©tect√©e**
   - V√©rifier le titre exact de la fen√™tre
   - S'assurer que DOFUS est en mode fen√™tr√©
   - Ajuster les param√®tres de capture dans EnvironmentConfig

2. **Performance GPU faible**
   - V√©rifier les drivers CUDA
   - Augmenter la taille de batch
   - Activer cudnn.benchmark = True

3. **R√©compenses instables**
   - Ajuster les poids du syst√®me de r√©compenses
   - Activer la normalisation des r√©compenses
   - Utiliser un learning rate plus faible

### D√©bogage

```python
# Activer le logging d√©taill√©
import logging
logging.basicConfig(level=logging.DEBUG)

# Sauvegarder les screenshots pour analyse
env_config.save_screenshots = True
env_config.screenshot_dir = "debug/screenshots"

# Activer les m√©triques detaill√©es
config.log_frequency = 1
```

## Contribution

Pour contribuer au syst√®me :
1. Cr√©er une branche feature
2. Impl√©menter les am√©liorations
3. Ajouter des tests
4. Mettre √† jour la documentation
5. Cr√©er une pull request

## Licence

Ce syst√®me est d√©velopp√© pour l'usage √©ducatif et de recherche. 
Respecter les conditions d'utilisation de DOFUS.

## Contact

Pour questions et support :
- Issues GitHub pour bugs et feature requests
- Documentation technique dans le code
- Exemples d'usage dans example_usage.py